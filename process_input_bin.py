# process_input_bin.py
import struct
import os
from typing import List, Tuple, Dict, Optional
import subprocess
import sys
from config import *
from bayesian_predictor import BayesianPredictor

class Processor:
    def __init__(self, name: str):
        self.name = name
        self.bayesian = BayesianPredictor(name)
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è Crumb'–æ–≤
        self.counters: Dict[int, int] = {}  # key: crumb_value, value: counter
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
        self.context_history: List[int] = []
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_matches_actualization = 0
        self.total_matches_others = 0
        self.total_first_prediction_hits = 0
        self.total_crumbs = 0
        self.total_bayesian_hits = 0
        
        # –§–∞–π–ª—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.state_file = os.path.join(OUTPUT_DIR, f"{name}.bin")  # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ output
        self.stats_file = os.path.join(STATS_DIR, f"{name}_matches.bin")  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ stats

    def process_crumb(self, crumb: int) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ Crumb'–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –±–∞–π–µ—Å–æ–≤—Å–∫–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º"""
        self._increment_total_crumbs()
        
        # –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        current_context = self._get_current_context()
        
        # –®–∞–≥ 1: –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
        prediction_result = self._try_enhanced_prediction(crumb, current_context)
        if prediction_result is not None:
            predicted_crumb, probability, confidence, method = prediction_result
            self.total_bayesian_hits += 1
            
            # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            self.bayesian.record_prediction_result(predicted_crumb, crumb, current_context)
            
            # –¢–∞–∫–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
            self._standard_processing(crumb, is_predicted=True)
            
            # –û–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            self._update_context_history(crumb)
            return
        
        # –®–∞–≥ 2: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        self._standard_processing(crumb, is_predicted=False)
        
        # –û–±–Ω–æ–≤–∏—Ç—å –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.bayesian.update_stats(crumb, True)
        self.bayesian.update_context(crumb)
        
        # –û–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self._update_context_history(crumb)

    def _try_enhanced_prediction(self, current_crumb: int, context: List[int]) -> Optional[Tuple]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
        available_crumbs = self._get_available_crumbs()
        if not available_crumbs:
            return None
            
        return self.bayesian.predict_next(context, available_crumbs)

    def _get_available_crumbs(self) -> List[int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Crumb'–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ø-N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤
        sorted_counters = sorted(self.counters.items(), key=lambda x: x[1], reverse=True)
        return [crumb for crumb, count in sorted_counters[:min(50, len(sorted_counters))]]

    def _get_current_context(self) -> List[int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N Crumb'–æ–≤ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        return self.context_history[-CONTEXT_DEPTH:] if self.context_history else []

    def _update_context_history(self, crumb: int) -> None:
        """–û–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        self.context_history.append(crumb)
        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≥–ª—É–±–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        if len(self.context_history) > CONTEXT_DEPTH * 3:
            self.context_history = self.context_history[-CONTEXT_DEPTH * 2:]

    def _standard_processing(self, crumb: int, is_predicted: bool) -> None:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ Crumb'–∞"""
        # –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—á–µ—Ç—á–∏–∫–∏
        sorted_counters = sorted(self.counters.items(), key=lambda x: x[1], reverse=True)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–æ–Ω—É –∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏
        actualization_count = max(1, int(len(sorted_counters) * ACTUALIZATION_RATIO))
        actualization_zone = dict(sorted_counters[:actualization_count])
        other_zone = dict(sorted_counters[actualization_count:])
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–æ–Ω—É –∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏
        if crumb in actualization_zone:
            self._increment_stat_actualization()
            self._increment_counter(crumb, PREDICT_INCREMENT)
            if self.total_crumbs == 1:
                self._increment_first_prediction()
            return
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏
        if crumb in other_zone:
            self._increment_stat_others()
            self._increment_counter(crumb, INCREMENT)
            return
        
        # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Å—á–µ—Ç—á–∏–∫
        self.counters[crumb] = 1

    def _increment_counter(self, crumb: int, increment: int) -> None:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç —Å—á–µ—Ç—á–∏–∫–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ"""
        current_value = self.counters.get(crumb, 0)
        new_value = current_value + increment
        
        if new_value > MAX_COUNTER_VALUE:
            print(f"‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–µ–ª —Å—á–µ—Ç—á–∏–∫–∞ {crumb}: {current_value} -> {new_value}")
            print(f"üîß –î–µ–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤ –Ω–∞ 2...")
            self._reset_counters()
            self.counters[crumb] = current_value // 2 + increment
        else:
            self.counters[crumb] = new_value

    def _reset_counters(self) -> None:
        """–°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–æ–≤ - –¥–µ–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ 2"""
        reset_count = 0
        for key in list(self.counters.keys()):
            old_value = self.counters[key]
            new_value = max(1, self.counters[key] // 2)
            self.counters[key] = new_value
            if old_value != new_value:
                reset_count += 1
        
        if reset_count > 0:
            print(f"üîÑ –°–±—Ä–æ—à–µ–Ω–æ {reset_count} —Å—á–µ—Ç—á–∏–∫–æ–≤ (–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ 2)")

    def _increment_stat_actualization(self) -> None:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∑–æ–Ω—ã –∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if self.total_matches_actualization >= MAX_STATS_VALUE:
            self._reset_stats()
        self.total_matches_actualization += 1

    def _increment_stat_others(self) -> None:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–æ–Ω"""
        if self.total_matches_others >= MAX_STATS_VALUE:
            self._reset_stats()
        self.total_matches_others += 1

    def _increment_first_prediction(self) -> None:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        if self.total_first_prediction_hits >= MAX_STATS_VALUE:
            self._reset_stats()
        self.total_first_prediction_hits += 1

    def _increment_total_crumbs(self) -> None:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ crumb'–æ–≤"""
        if self.total_crumbs >= MAX_STATS_VALUE:
            self._reset_stats()
        self.total_crumbs += 1

    def _reset_stats(self) -> None:
        """–°–±—Ä–æ—Å –≤—Å–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ - –¥–µ–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ 2"""
        print(f"üîÑ –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ 2):")
        print(f"   total_matches_actualization: {self.total_matches_actualization} -> {self.total_matches_actualization // 2}")
        print(f"   total_matches_others: {self.total_matches_others} -> {self.total_matches_others // 2}")
        print(f"   total_first_prediction_hits: {self.total_first_prediction_hits} -> {self.total_first_prediction_hits // 2}")
        print(f"   total_crumbs: {self.total_crumbs} -> {self.total_crumbs // 2}")
        print(f"   total_bayesian_hits: {self.total_bayesian_hits} -> {self.total_bayesian_hits // 2}")
        
        self.total_matches_actualization = max(1, self.total_matches_actualization // 2)
        self.total_matches_others = max(1, self.total_matches_others // 2)
        self.total_first_prediction_hits = max(1, self.total_first_prediction_hits // 2)
        self.total_crumbs = max(1, self.total_crumbs // 2)
        self.total_bayesian_hits = max(1, self.total_bayesian_hits // 2)

    def save_state(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        os.makedirs(STATS_DIR, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –≤ output
        with open(self.state_file, 'wb') as f:
            for key, value in self.counters.items():
                f.write(struct.pack(">II", key, value))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ stats
        with open(self.stats_file, 'wb') as f:
            stats = [
                (0, self.total_matches_actualization),
                (1, self.total_matches_others),
                (2, self.total_first_prediction_hits),
                (3, self.total_crumbs),
                (4, self.total_bayesian_hits)
            ]
            for key, value in stats:
                f.write(struct.pack(">II", key, value))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–π–µ—Å–æ–≤—Å–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–≤–∫–ª—é—á–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏)
        self.bayesian.save_state()

    def load_state(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        os.makedirs(STATS_DIR, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –∏–∑ output
        if os.path.exists(self.state_file):
            self.counters.clear()
            with open(self.state_file, 'rb') as f:
                data = f.read()
                for i in range(0, len(data), 8):
                    key, value = struct.unpack(">II", data[i:i+8])
                    self.counters[key] = value
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ stats
        if os.path.exists(self.stats_file):
            with open(self.stats_file, 'rb') as f:
                data = f.read()
                for i in range(0, len(data), 8):
                    key, value = struct.unpack(">II", data[i:i+8])
                    if key == 0:
                        self.total_matches_actualization = value
                    elif key == 1:
                        self.total_matches_others = value
                    elif key == 2:
                        self.total_first_prediction_hits = value
                    elif key == 3:
                        self.total_crumbs = value
                    elif key == 4:
                        self.total_bayesian_hits = value
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–π–µ—Å–æ–≤—Å–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–≤–∫–ª—é—á–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏)
        self.bayesian.load_state()


def read_chunk(file, chunk_size: int) -> bytes:
    """–ß—Ç–µ–Ω–∏–µ chunk'–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
    chunk = file.read(chunk_size)
    return chunk if chunk else b''


def process_chunk_beginning(chunk: bytes, processor: Processor) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ chunk'–∞ —Å –Ω–∞—á–∞–ª–∞"""
    for i in range(0, len(chunk), CRUMB_SIZE):
        crumb_bytes = chunk[i:i + CRUMB_SIZE]
        # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(crumb_bytes) < CRUMB_SIZE:
            crumb_bytes += b'\x00' * (CRUMB_SIZE - len(crumb_bytes))
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ (big-endian)
        crumb_value = int.from_bytes(crumb_bytes, byteorder='big')
        processor.process_crumb(crumb_value)


def process_chunk_inverse(chunk: bytes, processor: Processor) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ chunk'–∞ —Å –∫–æ–Ω—Ü–∞"""
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞–π—Ç—ã –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    reversed_chunk = chunk[::-1]
    
    for i in range(0, len(reversed_chunk), CRUMB_SIZE):
        crumb_bytes = reversed_chunk[i:i + CRUMB_SIZE]
        # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(crumb_bytes) < CRUMB_SIZE:
            crumb_bytes += b'\x00' * (CRUMB_SIZE - len(crumb_bytes))
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ (big-endian)
        crumb_value = int.from_bytes(crumb_bytes, byteorder='big')
        processor.process_crumb(crumb_value)


def run_audio_stream():
    """–ó–∞–ø—É—Å–∫ –∞—É–¥–∏–æ –º–æ–¥—É–ª—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    print("\n" + "="*50)
    print("–ó–∞–ø—É—Å–∫ –∞—É–¥–∏–æ –º–æ–¥—É–ª—è...")
    print("="*50)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª audio_stream.py
        if os.path.exists("audio_stream.py"):
            subprocess.run([sys.executable, "audio_stream.py"])
        else:
            print("–§–∞–π–ª audio_stream.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∞—É–¥–∏–æ –º–æ–¥—É–ª—è: {e}")


def main():
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    os.makedirs("input", exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(STATS_DIR, exist_ok=True)
    os.makedirs(HIERARCHICAL_DIR, exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if not os.path.exists(INPUT_FILE):
        print(f"–û—à–∏–±–∫–∞: –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª input/input.bin —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        return
    
    # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    print("–¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"  CHUNK_SIZE: {CHUNK_SIZE}")
    print(f"  CRUMB_SIZE: {CRUMB_SIZE}")
    print(f"  MAX_COUNTER_VALUE: {MAX_COUNTER_VALUE} (0x{MAX_COUNTER_VALUE:08X})")
    print(f"  MAX_STATS_VALUE: {MAX_STATS_VALUE} (0x{MAX_STATS_VALUE:08X})")
    print(f"  PREDICT_INCREMENT: {PREDICT_INCREMENT}")
    print(f"  INCREMENT: {INCREMENT}")
    print(f"  ACTUALIZATION_RATIO: {ACTUALIZATION_RATIO}")
    print(f"  –ë–∞–∑–æ–≤—ã–µ –±–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"    ALPHA: {BAYES_ALPHA}, BETA: {BAYES_BETA}")
    print(f"    CONFIDENCE_THRESHOLD: {CONFIDENCE_THRESHOLD}")
    print(f"    MIN_OBSERVATIONS: {MIN_OBSERVATIONS}")
    print(f"  –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"    –í–∫–ª—é—á–µ–Ω–æ: {HIERARCHICAL_ENABLED}")
    print(f"    GROUP_SIZE: {GROUP_SIZE}")
    print(f"    CONTEXT_DEPTH: {CONTEXT_DEPTH}")
    print(f"    ALPHA_PRIOR: {HIERARCHICAL_ALPHA_PRIOR}")
    print(f"    BETA_PRIOR: {HIERARCHICAL_BETA_PRIOR}")
    print()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
    beginning_processor = Processor("begin")
    inverse_processor = Processor("inverse")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    beginning_processor.load_state()
    inverse_processor.load_state()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
    try:
        with open(INPUT_FILE, 'rb') as file:
            chunk_count = 0
            
            while True:
                chunk = read_chunk(file, CHUNK_SIZE)
                if not chunk:
                    break
                
                print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ chunk #{chunk_count + 1}, —Ä–∞–∑–º–µ—Ä: {len(chunk)} –±–∞–π—Ç")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ beginning –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º
                process_chunk_beginning(chunk, beginning_processor)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ inverse –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º
                process_chunk_inverse(chunk, inverse_processor)
                
                chunk_count += 1
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                if chunk_count % 10 == 0:
                    beginning_processor.save_state()
                    inverse_processor.save_state()
                    
                    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–∞—Ö –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ
                    if beginning_processor.counters:
                        max_begin = max(beginning_processor.counters.values())
                        print(f"  Beginning: —Å—á–µ—Ç—á–∏–∫–æ–≤={len(beginning_processor.counters)}, –º–∞–∫—Å={max_begin}")
                    if inverse_processor.counters:
                        max_inverse = max(inverse_processor.counters.values())
                        print(f"  Inverse: —Å—á–µ—Ç—á–∏–∫–æ–≤={len(inverse_processor.counters)}, –º–∞–∫—Å={max_inverse}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        beginning_processor.save_state()
        inverse_processor.save_state()
        
        print("\n" + "="*50)
        print("–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("="*50)
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ chunk'–æ–≤: {chunk_count}")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ Crumb'–æ–≤: {beginning_processor.total_crumbs + inverse_processor.total_crumbs}")
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–¶–ï–°–°–û–†–û–í:")
        print("\nBeginning –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä:")
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ Crumb'–æ–≤: {beginning_processor.total_crumbs}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤: {len(beginning_processor.counters)}")
        print(f"  –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–æ–Ω–µ –∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏: {beginning_processor.total_matches_actualization}")
        print(f"  –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–æ–Ω–∞—Ö: {beginning_processor.total_matches_others}")
        print(f"  –ü–æ–ø–∞–¥–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {beginning_processor.total_first_prediction_hits}")
        print(f"  –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –ø–æ–ø–∞–¥–∞–Ω–∏—è: {beginning_processor.total_bayesian_hits}")
        
        print("\nInverse –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä:")
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ Crumb'–æ–≤: {inverse_processor.total_crumbs}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤: {len(inverse_processor.counters)}")
        print(f"  –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–æ–Ω–µ –∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏: {inverse_processor.total_matches_actualization}")
        print(f"  –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–æ–Ω–∞—Ö: {inverse_processor.total_matches_others}")
        print(f"  –ü–æ–ø–∞–¥–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {inverse_processor.total_first_prediction_hits}")
        print(f"  –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –ø–æ–ø–∞–¥–∞–Ω–∏—è: {inverse_processor.total_bayesian_hits}")
        
        # –í—ã–≤–æ–¥ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print("\n" + "="*50)
        print("–ë–ê–ô–ï–°–û–í–°–ö–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print("="*50)
        beginning_processor.bayesian.print_stats()
        print()
        inverse_processor.bayesian.print_stats()
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        print("\n" + "="*50)
        print("–û–ë–©–ê–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨:")
        print("="*50)
        total_crumbs = beginning_processor.total_crumbs + inverse_processor.total_crumbs
        total_bayesian_hits = beginning_processor.total_bayesian_hits + inverse_processor.total_bayesian_hits
        
        if total_crumbs > 0:
            bayesian_efficiency = total_bayesian_hits / total_crumbs
            print(f"–û–±—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–∞–π–µ—Å–æ–≤—Å–∫–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {bayesian_efficiency:.2%}")
        
        print(f"\nüíæ –§–ê–ô–õ–´ –°–û–•–†–ê–ù–ï–ù–´:")
        print(f"  –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ '{OUTPUT_DIR}':")
        print(f"    - begin.bin, inverse.bin")
        print(f"  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ '{STATS_DIR}':")
        print(f"    - begin_matches.bin, inverse_matches.bin")
        print(f"    - begin_bayes.bin, inverse_bayes.bin")
        print(f"  –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –≤ '{HIERARCHICAL_DIR}':")
        print(f"    - begin_groups.bin, inverse_groups.bin")
        print(f"    - begin_context.bin, inverse_context.bin")
        
        # –ó–∞–ø—É—Å–∫ –∞—É–¥–∏–æ –º–æ–¥—É–ª—è –≤ –∫–æ–Ω—Ü–µ
        run_audio_stream()
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()